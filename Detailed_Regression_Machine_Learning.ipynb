{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python - Regression Algorithms\n",
    "\n",
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sl.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a predictive model that can predict the price of homes based on some variables (characteristics) on several homes in a Boston neighborhood. Based on a series of attributes, we will indicate a numeric value through regression.\n",
    "\n",
    "Dataset: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Metrics for Regression Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need metrics to evaluate the outcome of a regression model. Therefore, the choice of an algorithm is that it will define which metric will be used to measure your performance. scikit-learn does not implement all performance metrics*\n",
    "\n",
    "- Mean Squared Error (MSE)  -  Average Square Error\n",
    "- Root Mean Squared Error (RMSE)  -  Square Root MSE\n",
    "- Mean Absolute Error (MAE)  -  Average Absolute Error\n",
    "- R Squared (R²)  -  Coefficient of Determination\n",
    "- Adjusted R Squared (R²)  -  R Adjusted\n",
    "- Mean Square Percentage Error (MSPE)\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "- Root Mean Squared Logarithmic Error (RMSLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MSE - the magnitude of model error\n",
    "\n",
    "Maybe it's the most accessible metric to understand. N is the number of observations in the dataset, the sum, Yi of the historical values that have already been collected, and y^ is the model's prediction. We square it, so we don't have negative values.\n",
    "\n",
    "The algorithm is fed X (input predictor variable) and Y (output target variable) during algorithm training. The algorithm learns mathematical relationships and makes a prediction defined as y^.\n",
    "\n",
    "After the prediction, we calculate the difference between the model forecast and the historical value of the target variable. This calculation will return an error rate - Average quadratic error. Depending on the value of the MSE, we were able to verify whether or not the model performs well through the error rate, that is, the smaller - the better the model.\n",
    "\n",
    "It is perhaps the simplest and most common metric for regression evaluation and probably the least useful. The MSE misunderstands the average square error of our predictions. For each point, it calculates the square difference between the predictions and the actual value of the target variable and then calculates the average of those values.\n",
    "\n",
    "The higher this value, the worse the model is. This value will never be negative since individual prediction errors square us, but it would be zero for a perfect model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE - Mean Squared Error\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = LinearRegression()\n",
    "# Traning model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "# Result\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is::\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the predictions, we Y_pred call the mean_squared_error that will apply the MSE. The function receives the parameter Y_test which is the actual value of Y and Y_pred we calculated in the forecast set.\n",
    "We have the MSE for this model of 28.53. The ideal is that we do a work of pre-processing, the transformation of variables, standardization in this data set to reduce the value of the MSE. The smaller the MSE, the better our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the MSE or RMSE depending on the type of interpretation we want for the final result. We should compare two models using the same metric*.\n",
    "\n",
    "To get to the RMSE result, calculate the square root of the MSE that we figured earlier with the mean_squared_error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, we will use absolute values, usually when we have outliers in the dataset. For this, we use the mean fundamental error, that is, the sum of the absolute difference between forecasts and actual values. We use absolute values instead of the squared error of the MSE to calculate.\n",
    "\n",
    "The value of 0 indicates no error - the perfect prediction is, which is very rare to happen. Our job as a Data Scientist is to reduce this rate as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE - Mean Absolute Error\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = LinearRegression()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "# Metric Result\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "print(\"The MAE of the model is\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Linear Regression, we have the MAE of 3.45, but be careful - we can't compare the MAE to the MSE of 28.5! We should compare different models but with equal metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. R² - Coefficient of Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of R² is that it returns a coefficient that goes from 0 to 1. As higher, the better the model is, unlike the other metrics where we interpret an error rate below.\n",
    "\n",
    "This metric reflects the level of accuracy of the predictions relative to the values observed through the r2_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R²\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = LinearRegression()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "# MetricResult\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(\"The R2 of the model is:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is the most straightforward algorithm of all, where we have two main variants of the regression:\n",
    "\n",
    "- Simple Linear Regression: an input variable\n",
    "- Multiple Linear Regression: Many Input Variables\n",
    "\n",
    "Regression assumes that the data are in Normal Distribution. The variables are relevant for the construction of the model. They are not collinear, that is, variables with high correlation - it is up to the Food Scientist the algorithm with really relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression# Loading data\n",
    "file = 'boston-houses.csv'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = LinearRegression()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "# Metric Result\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we deal with multiple linear regression, and we are dealing with more than one predictor variable. We use the LinearRegression algorithm of the linear_model module, load the data, place the predictor variables in X, the target variables in Y, divide the random form into training and test data, and then create the Linear Regression model and train the relationships of the training data.\n",
    "\n",
    "Finally, we make the predictions. Once the predictions are made, we put them into the MSE metric to calculate the error rate of the forecasts.\n",
    "\n",
    "With this Linear Regression Algorithm, we have an MSE error rate of 28.53% without any pre-processing of the data. Can we improve this error rate just by changing the algorithm? We could also apply normalization, standardization, variable transformation, variable selection, cross-validation - to focus on the process. We will change only the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a Linear Regression algorithm where the Loss Function is modified to minimize the complexity of the model. A Loss Function is the Cost function or Error function.\n",
    "\n",
    "When we build the model, we need to automate the process. To automate the process, we need to use the Gradient Descent algorithm. To use The Gradient Descent, we have to use a Cost Function to optimize this function and reduce the Cost Function, consequently reducing the model's error rate.\n",
    "\n",
    "Machine Learning is an optimization problem. We want to optimize the cost function, reduce the error rate of the model. Therefore, we have the Linear Regression algorithm, the Optimization algorithm, a Cost Function, which puts it all together and trains the model to find the best mathematical relationship between input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = Ridge()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "# Metric Result\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE of the Ridge Regression model is 29.29%. That is, we can not improve the performance of the model. We just worsened by changing the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regression is a modification of linear regression, and like Ridge Regression, the Loss Function is modified to minimize model complexity.\n",
    "\n",
    "The algorithm does change the penalty rate to have a more straightforward cost function to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing with the train_test_split()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = Lasso()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictiosn\n",
    "Y_pred = model.predict(X_test)\n",
    "# Metrics Result\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE with Lasso Regression result was 33.39. By changing the algorithm, we are increasing the error rate. We use the MSE to make this comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ElasticNet Regression (Rigde ~ Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet is a form of regression regularization that combines the properties of ridge and lasso regression. The objective is to minimize the complexity of the model by penalizing the model using the sum of squares of the coefficients.\n",
    "\n",
    "Therefore, all algorithms seen earlier are only variations - Linear Regression, a Ridge modification, and Lasso and now ElasticNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet Regression\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing\n",
    "train_test_split()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = ElasticNet()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "# MetricResult\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use KNN for both classification and regression, just use the KNeighborsRegressor algorithm, that is, it is the algorithm for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values#\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = KNeighborsRegressor()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "# Metric result = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE with KNN Regressor was 47.70. We made the cost function much worse. KNN is a much simpler algorithm, and perhaps for this dataset, it is not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use it in both classification and regression categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = DecisionTreeRegressor()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions Y_pred = model.predict(X_test)\n",
    "# Metric Result\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE with CART was 30.03. We can already see an improvement in the lower cost function. Generally, decision trees perform excellently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. SVM - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the SVC for classification and the SVR for regression. The rest is the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR \n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Divides the data into training and testingX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# Creating model\n",
    "model = SVR()# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Making predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "# Metric esult\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE with SVR was 93.21! Worst performance ever. SVM is a much more complex algorithm, and because no processing has been done on the data, the algorithm rejects the data as-is.\n",
    "\n",
    "We must do a little better treatment. A more complex algorithm that performs good results requires much more pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization  -  Parameter Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing a regression model follows the same rules for classification, with no significant difference.\n",
    "\n",
    "All machine learning algorithms are parameterized, which means you can adjust predictive model performance by tuning parameters.\n",
    "\n",
    "The goal is to find the best combination of the parameters in each machine learning algorithm. This process is also called hyperparameter optimization. scikit-learn offers two methods for automatic parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Grid Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method methodically performs combinations between all algorithm parameters, creating a grid.\n",
    "\n",
    "Let's try this method using the Ridge Regression algorithm, so we can see in practice how we can optimize this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Parameter Tuning\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Setting the values that will be tested\n",
    "alpha_values = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "grid_values = dict(alpha = alpha_values)\n",
    "# Creating model\n",
    "model = Ridge()\n",
    "# Creating grid\n",
    "grid = GridSearchCV(estimator = model, param_grid = grid_values)\n",
    "grid.fit(X, Y)\n",
    "# Print the result of the best parameter for the algorithm\n",
    "print(\"Best Model Parameters:\", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a grid with the parameters we want to try and make a dictionary called grid_values. We started the Ridge model and then called GridSearchCV to test the combination of parameters.\n",
    "\n",
    "The output will be the best parameters for the Ridge algorithm with GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method generates samples of algorithm parameters from a uniform random distribution for a fixed number of interactions.\n",
    "\n",
    "A model is built and tested for each combination of parameters.\n",
    "This example shows that the value very close to 1 for the alpha parameter will present the best results.\n",
    "\n",
    "Therefore, we compared the models according to the metrics and chose the one that has the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search Parameter Tuning\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Setting parameters\n",
    "grid_values = {'alpha': uniform()}\n",
    "seed = 7Creating model\n",
    "model = Ridge()\n",
    "iterations = 100\n",
    "rsearch = RandomizedSearchCV(estimator = model, \n",
    "                             param_distributions = grid_values, \n",
    "                             n_iter = iterations, \n",
    "                             random_state = seed)\n",
    "rsearch.fit(X, Y)\n",
    "# Result\n",
    "print(\"Best Model Parameters:\", rsearch.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we compared the models according to the metrics and chose the one that has the best value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving result\n",
    "# Import modules\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import pickle\n",
    "# Loading data\n",
    "file = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(file, delim_whitespace = True, names = columns)\n",
    "array = data.values\n",
    "# Separating the array into input and output components\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "# Setting parameters\n",
    "test_size = 0.35\n",
    "seed = 7\n",
    "# Divides the data into training and testing \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5) = seed)\n",
    "# Creating model\n",
    "model = Ridge()\n",
    "# Training model\n",
    "model.fit(X_train, Y_train)\n",
    "# Saving model\n",
    "file = 'final_regression_model.sav'\n",
    "pickle.dump(model, open(file, 'wb'))\n",
    "print(\"Model saved!\")\n",
    "# Loading model\n",
    "final_regressor_model = pickle.load(open(file, 'rb'))\n",
    "print(\"Model saved!\")\n",
    "# Making Predictions \n",
    "Y_pred = final_regressor_model.predict(X_test)\n",
    "# Metric Result\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"The MSE of the model is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we had an overview of the machine learning process, that is, building the models; the focus here was not to detail how each algorithm works. Our goal was to understand the process.\n",
    "\n",
    "The data scientist's job is to master as much as possible everything we've seen from pre-processing, model selection, performance metrics, model optimization, and forecasting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
